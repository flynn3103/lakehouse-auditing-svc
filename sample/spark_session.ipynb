{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/local/lib/python311.zip', '/usr/local/lib/python3.11', '/usr/local/lib/python3.11/lib-dynload', '', '/home/appuser/.local/lib/python3.11/site-packages', '/usr/local/lib/python3.11/site-packages', '/', '/', '/', '/app', '/app', '/home/appuser/.local/lib/python3.11/site-packages/setuptools/_vendor', '/app']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of lakehouse_engine to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "\n",
    "from test.utils.exec_env_helpers import ExecEnvHelpers\n",
    "from test.utils.dataframe_helpers import DataframeHelpers\n",
    "# Verify the updated sys.path\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 10:16:36,207 — lakehouse_engine.core.exec_env — INFO — Using the following default configs you may want to override them for your job: {'spark.databricks.delta.optimizeWrite.enabled': True, 'spark.sql.adaptive.enabled': True, 'spark.databricks.delta.merge.enableLowShuffle': True}\n",
      "2024-12-31 10:16:36,211 — lakehouse_engine.core.exec_env — INFO — Final config is: {'spark.databricks.delta.optimizeWrite.enabled': True, 'spark.sql.adaptive.enabled': True, 'spark.databricks.delta.merge.enableLowShuffle': True, 'spark.master': 'local[2]', 'spark.driver.memory': '2g', 'spark.sql.warehouse.dir': 'file:///app/tests/lakehouse/spark-warehouse/', 'spark.sql.shuffle.partitions': '2', 'spark.sql.extensions': 'io.delta.sql.DeltaSparkSessionExtension', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.delta.catalog.DeltaCatalog', 'spark.jars.packages': 'io.delta:delta-spark_2.12:3.2.0,org.xerial:sqlite-jdbc:3.45.3.0,com.databricks:spark-xml_2.12:0.18.0', 'spark.jars.excludes': 'net.sourceforge.f2j:arpack_combined_all', 'spark.sql.sources.parallelPartitionDiscovery.parallelism': '2', 'spark.sql.legacy.charVarcharAsString': True}\n",
      "2024-12-31 10:16:36,212 — lakehouse_engine.core.exec_env — INFO — Creating a new Spark Session\n",
      "/home/appuser/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/appuser/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/appuser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/appuser/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.xerial#sqlite-jdbc added as a dependency\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-181e5454-4543-4b19-8e31-f407769359b0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.xerial#sqlite-jdbc;3.45.3.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound com.databricks#spark-xml_2.12;0.18.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 420ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.18.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.xerial#sqlite-jdbc;3.45.3.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-181e5454-4543-4b19-8e31-f407769359b0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/14ms)\n",
      "24/12/31 10:16:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-12-31 10:16:45,294 — test.utils.dataframe_helpers — INFO — Starting the script to read data using DataframeHelpers.\n",
      "2024-12-31 10:16:45,307 — lakehouse_engine.utils.schema_utils — INFO — No schema was provided... skipping enforce schema\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create single execution environment session.\"\"\"\n",
    "ExecEnvHelpers.prepare_exec_env('2g')\n",
    "# Example usage\n",
    "DataframeHelpers._logger.info(\n",
    "    \"Starting the script to read data using DataframeHelpers.\"\n",
    ")\n",
    "location = \"/app/test/data/test.csv\"  # Replace with the actual file path\n",
    "try:\n",
    "    df = DataframeHelpers.read_from_file(location)\n",
    "    df.show()  # Display the DataFrame content\n",
    "except Exception as e:\n",
    "    DataframeHelpers._logger.error(f\"Error reading the file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
